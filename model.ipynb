{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction and Set-up\n","\n","TensorFlow Datasets has many datasets that can be loaded and be used to learn more about image classification and various computer vision machine learning pipelines. It is a great way to learn more about TensorFlow and the architecture of computer vision models in general. This tutorial will go over the importance of data exploration as well as all the steps for a image classification machine learning problem."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":false,"trusted":true},"outputs":[],"source":["import re\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","# from kaggle_datasets import KaggleDatasets\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import scipy\n","import gc\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Device:', tpu.master())\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","    strategy = tf.distribute.get_strategy()\n","print('Number of replicas:', strategy.num_replicas_in_sync)\n","    \n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Initial Exploration\n","\n","We will be using the TFDS Malaria dataset. The Malaria dataset is contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells from the thin blood smear slide images of segmented cells. The original data source is from [NIH](https://lhncbc.nlm.nih.gov/publication/pub9932). A big aspect of machine learning is data processing. Feature engineering and normalizing data is important. Correctly formatted data will help the model train better and make better inferences about the data."]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","IMAGE_SIZE = [180, 180]\n","EPOCHS = 25"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the data\n","\n","We'll be using TFDS Malaria dataset for our example. It is quite easy to load the data using TFDS API."]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-11 00:07:51.885578: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1mDownloading and preparing dataset 337.08 MiB (download: 337.08 MiB, generated: Unknown size, total: 337.08 MiB) to /home/allan/tensorflow_datasets/malaria/1.0.0...\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["Dl Completed...: 0 url [00:00, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:15<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:16<?, ? url/s]\n","Dl Completed...:   0%|          | 0/1 [00:22<?, ? url/s]\n","Extraction completed...: 0 file [00:33, ? file/s]\n","Dl Size...:   5%|â–         | 16/337 [00:33<11:21,  2.12s/ MiB]\n","Dl Completed...:   0%|          | 0/1 [00:33<?, ? url/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds, info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmalaria\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:649\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[1;32m    644\u001b[0m     name,\n\u001b[1;32m    645\u001b[0m     data_dir,\n\u001b[1;32m    646\u001b[0m     builder_kwargs,\n\u001b[1;32m    647\u001b[0m     try_gcs,\n\u001b[1;32m    648\u001b[0m )\n\u001b[0;32m--> 649\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:508\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m    507\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 508\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:691\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    697\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    698\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1547\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1546\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1547\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1554\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[1;32m   1555\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[1;32m   1556\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1557\u001b[0m )\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/datasets/malaria/malaria_dataset_builder.py:55\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Define Splits.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m   path \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     58\u001b[0m       tfds\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mSplitGenerator(\n\u001b[1;32m     59\u001b[0m           name\u001b[38;5;241m=\u001b[39mtfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTRAIN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m       ),\n\u001b[1;32m     64\u001b[0m   ]\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:688\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m    687\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:832\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m    831\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m--> 832\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[1;32m    833\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/promise/promise.py:511\u001b[0m, in \u001b[0;36mPromise.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# type: (Optional[float]) -> T\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEFAULT_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_settled_value(_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/promise/promise.py:506\u001b[0m, in \u001b[0;36mPromise._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# type: (Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/promise/promise.py:502\u001b[0m, in \u001b[0;36mPromise.wait\u001b[0;34m(cls, promise, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mcls\u001b[39m, promise, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# type: (Promise, Optional[float]) -> None\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[43masync_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/promise/async_.py:117\u001b[0m, in \u001b[0;36mAsync.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m promise\u001b[38;5;241m.\u001b[39mis_pending:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# We return if the promise is already\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# fulfilled or rejected\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.virtualenvs/DSC/lib/python3.10/site-packages/promise/schedulers/immediate.py:25\u001b[0m, in \u001b[0;36mImmediateScheduler.wait\u001b[0;34m(self, promise, timeout)\u001b[0m\n\u001b[1;32m     22\u001b[0m     e\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m     24\u001b[0m promise\u001b[38;5;241m.\u001b[39m_then(on_resolve_or_reject, on_resolve_or_reject)\n\u001b[0;32m---> 25\u001b[0m waited \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m waited:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n","File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["ds, info = tfds.load('malaria', split='train', shuffle_files=True, with_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Num classes: \" + str(info.features['label'].num_classes))\n","print(\"Class names: \" + str(info.features['label'].names))"]},{"cell_type":"markdown","metadata":{},"source":["As expected, we have two different classes of images: a \"parasitized\" class and an \"uninfected\" class."]},{"cell_type":"markdown","metadata":{},"source":["## Visualize the data\n","\n","Let's use the TFDS API to visualize how our images look like."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vis = tfds.visualization.show_examples(ds, info)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature extraction\n","\n","Let's convert our images and labels into numpy arrars for our initial analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_images = []\n","train_labels = []\n","\n","for example in ds:\n","    train_images.append(example['image'].numpy())\n","    train_labels.append(example['label'].numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_images = np.array(train_images)\n","train_labels = np.array(train_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Image:\")\n","print(train_images[0])\n","print(\"Label: \" + str(train_labels[0]))"]},{"cell_type":"markdown","metadata":{},"source":["As we see in our visualizations, not all of the images are of the same size. Additionally, our images, and for most computer vision problems, images will be a 3-channel matrix, meaning 3 matrices are stacked on top of each, one for each color of RGB. Sometimes, features of our image like size, length, and shape can be strong correlations with our labels.\n","\n","Let's evaluate the length of our images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images_flattened = [x.flatten().astype('float64') for x in train_images]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img_lengths = []\n","\n","for img in images_flattened:\n","    img_lengths.append(len(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img_lengths = np.array(img_lengths)"]},{"cell_type":"markdown","metadata":{},"source":["Let's see our the lengths of the images identified as \"uninfected\" differ from the lengths of the images identifies as \"parasitized\"."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["uninfected_lengths = img_lengths[train_labels]\n","parasitized_lengths = img_lengths[train_labels == 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["scipy.stats.describe(uninfected_lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.scatter(np.arange(len(uninfected_lengths)), uninfected_lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["np.unique(uninfected_lengths)"]},{"cell_type":"markdown","metadata":{},"source":["We see that for the uninfected images, the length of the flattened image array is either 41745 or 54165. Now let's see the lengths of the parasitized images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["scipy.stats.describe(parasitized_lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.scatter(np.arange(len(parasitized_lengths)), parasitized_lengths)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["np.unique(parasitized_lengths)"]},{"cell_type":"markdown","metadata":{},"source":["For the parasitized lengths, we see that images are a wide variety of lengths. For certain models, having a feature that corresponds with the label can be an issue as the model might assume that the length of an image corresponds with its classification. This will make it difficult to generalize the model as not all uninfected blood smear images are of the same size. To help prevent overfitting and to generalize our model, we will preprocess our images before inputing them.\n","\n","But first, let's clear some RAM so we don't run out of resources."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del ds\n","del info\n","del train_images\n","del train_labels\n","del images_flattened\n","del img_lengths\n","\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Model building\n","\n","## Loading images\n","\n","We first want to load our images into three different datasets: a training dataset, a validation dataset, and a training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32\n","IMAGE_SIZE = [200, 200]\n","\n","train_ds, val_ds, test_ds = tfds.load('malaria',\n","                                      split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],\n","                                      shuffle_files=True, as_supervised=True)"]},{"cell_type":"markdown","metadata":{},"source":["We will divide our data into 70:15:15 ratio. We can check that our ratios are correct by checking how many images are in each dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds).numpy()\n","print(\"Num training images: \" + str(NUM_TRAIN_IMAGES))\n","\n","NUM_VAL_IMAGES = tf.data.experimental.cardinality(val_ds).numpy()\n","print(\"Num validating images: \" + str(NUM_VAL_IMAGES))\n","\n","NUM_TEST_IMAGES = tf.data.experimental.cardinality(test_ds).numpy()\n","print(\"Num testing images: \" + str(NUM_TEST_IMAGES))"]},{"cell_type":"markdown","metadata":{},"source":["## Reshape image input\n","\n","Let's see the shapes of our images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for image, label in train_ds.take(1):\n","    print(\"Image shape: \", image.numpy().shape)\n","    print(\"Label: \", label.numpy())"]},{"cell_type":"markdown","metadata":{},"source":["Not all the images are of size (200, 200). Thankfully, TensorFlow Image API has a way to resize images by either cropping big pictures or padding smaller ones. Let's define our padding method."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def convert(image, label):\n","  image = tf.image.convert_image_dtype(image, tf.float32)\n","  return image, label\n","\n","def pad(image,label):\n","  image,label = convert(image, label)\n","  image = tf.image.resize_with_crop_or_pad(image, 200, 200)\n","  return image,label"]},{"cell_type":"markdown","metadata":{},"source":["We have to use `.map()` to apply our padding method to all of our images. While we are at it, we should batch our images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["padded_train_ds = (\n","    train_ds\n","    .cache()\n","    .map(pad)\n","    .batch(BATCH_SIZE)\n",") \n","\n","padded_val_ds = (\n","    val_ds\n","    .cache()\n","    .map(pad)\n","    .batch(BATCH_SIZE)\n",") "]},{"cell_type":"markdown","metadata":{},"source":["## Visualize padded images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_batch, label_batch = next(iter(padded_train_ds))\n","\n","def show_batch(image_batch, label_batch):\n","    plt.figure(figsize=(10,10))\n","    for n in range(25):\n","        ax = plt.subplot(5,5,n+1)\n","        plt.imshow(image_batch[n])\n","        if label_batch[n]:\n","            plt.title(\"uninfected\")\n","        else:\n","            plt.title(\"parasitized\")\n","        plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["show_batch(image_batch.numpy(), label_batch.numpy())"]},{"cell_type":"markdown","metadata":{},"source":["## Build our model\n","\n","Let's build our deep CNN. We will be using the TensorFlow Keras API for easy implementation.\n","\n","We'll create two blocks, one convolution block and one dense block so we won't have to repeat our code."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def conv_block(filters):\n","    block = tf.keras.Sequential([\n","        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n","        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.MaxPool2D()\n","    ]\n","    )\n","    \n","    return block\n","\n","def dense_block(units, dropout_rate):\n","    block = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units, activation='relu'),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(dropout_rate)\n","    ])\n","    \n","    return block"]},{"cell_type":"markdown","metadata":{},"source":["Now we'll define our model. We want our last layer to be a dense layer with a single node. The closer the value is to 1, the higher likelihood that the image is uninfected. Values closer to 0 indice a higher probability of being parasitized."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_model():\n","    model = tf.keras.Sequential([\n","        tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n","        \n","        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n","        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n","        tf.keras.layers.MaxPool2D(),\n","        \n","        conv_block(32),\n","        conv_block(64),\n","        \n","        conv_block(128),\n","        tf.keras.layers.Dropout(0.2),\n","        \n","        conv_block(256),\n","        tf.keras.layers.Dropout(0.2),\n","        \n","        tf.keras.layers.Flatten(),\n","        dense_block(512, 0.7),\n","        dense_block(128, 0.5),\n","        dense_block(64, 0.3),\n","        \n","        tf.keras.layers.Dense(1, activation='sigmoid')\n","    ])\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["As we are working on a binary classification problem, we will be using a binary crossentropy loss function. Additionally, this data, luckily, is balanced. This means that half of the images are parasitized and half the images are uninfected. Because we are working with a balanced dataset, we will be using AUC-ROC as our metric. To learn more about AUC-ROC, check out this [resource](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = build_model()\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=tf.keras.metrics.AUC(name='auc')\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Callbacks\n","\n","We want to define certain callbacks so that we have the best model without overfitting.\n","\n","One of the most important hyperparameters is the learning rate. A learning rate that is too high will prevent the model from converging. Conversely, a learning rate that is too slow will cause the training process to be too long and take up unnecessary resources. We'll be using an exponential decay function to change our learning rate for each epoch.\n","\n","The checkpoint and early stopping callback saves the best weights for the model and stops the model once it stops improving. This will slow down overfitting and save time."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"malaria_model.h5\",\n","                                                    save_best_only=True)\n","\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5,\n","                                                     restore_best_weights=True)\n","\n","def exponential_decay(lr0, s):\n","    def exponential_decay_fn(epoch):\n","        return lr0 * 0.1 **(epoch / s)\n","    return exponential_decay_fn\n","\n","exponential_decay_fn = exponential_decay(0.01, 20)\n","\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = model.fit(\n","    padded_train_ds, epochs=20,\n","    validation_data=padded_val_ds,\n","    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate results\n","\n","First let's preprocess our testing images."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["padded_test_ds = (\n","     test_ds\n","    .cache()\n","    .map(pad)\n","    .batch(BATCH_SIZE)\n",") "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.evaluate(padded_test_ds)"]},{"cell_type":"markdown","metadata":{},"source":["We see that our model has an AUC-ROC score of . A high AUC-ROC shows that our model works well at differentiating between parasitized and uninfected cells."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.summary()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
